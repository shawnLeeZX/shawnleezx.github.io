<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta content="Shawn" property="og:site_name">
  <link href="/favicon.png" rel="icon">

  
  <meta content="Research & Development" property="og:title">
  

  
  <meta content="article" property="og:type">
  

  
  <meta content="<h1>Shawn W. M. Li</h1> " property="og:description">
  

  
  <meta content="http://0.0.0.0:4000/research/" property="og:url">
  

  

  <meta property="og:image" content="">

  

  

  <title>Research &amp; Development - Shawn</title>
  <meta name="description" content="<h1>Shawn W. M. Li</h1> ">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
  <!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/css/bootstrap.min.css" integrity="sha384-GJzZqFGwb1QTTN6wy59ffF1BuGJpLSa9DkKMp0DgiMDm4iYMj70gZWKYbI706tWS" crossorigin="anonymous"> -->
  <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.slim.min.js"></script> -->
  <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script> -->
  <!-- <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script> -->
  <link rel="stylesheet" href="/css/main.css">

  <!-- Typesetting math using MathJax -->
  <!-- mathjax config similar to math.stackexchange -->
  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     jax: ["input/TeX", "output/HTML-CSS"],
     tex2jax: {
       inlineMath: [ ['$', '$'] ],
       // displayMath: [ ['$$', '$$']],
       processEscapes: true,
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     },
     messageStyle: "none",
     "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
   });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-44220731-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>



  <link href="/css/fonts/orbitron.css" rel="stylesheet" type="text/css">
  <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
  <link rel="canonical" href="http://0.0.0.0:4000/research/">
  <link rel="alternate" type="application/rss+xml" title="Shawn" href="http://0.0.0.0:4000/feed.xml">
</head>


  <body>
    <section>
  <nav class="navbar navbar-default navbar-expand-lg fixed-top">
    <div class="container">

      <!-- Brand and toggle get grouped for better mobile display -->
      <a class="navbar-brand" href="/">Shawn</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navBar">
        <span>Menu</span>
      </button>

      <div class="collapse navbar-collapse" align="right" id="navBar">
        <ul class="navbar-nav ms-auto">
          <li class="nav-item"><a class="nav-link" href="/about">About</a></li>
          <li class="nav-item"><a class="nav-link" href="/research">Research & Development</a></li>
          <li class="nav-item"><a class="nav-link" href="/blog">Blog</a></li>
          <!-- <li class="nav-item"><a class="nav-link" href="/bio">Biography</a></li> -->
          <!-- <li class="nav-item"><a class="nav-link" href="/essays">Essays</a></li> -->
          <li class="nav-item"><a class="nav-link" href="/community">Community</a></li>
        </ul>
      </div><!-- /.navbar-collapse -->

    </div>
  </nav>
</section>


    <section>
      <div class="container">
  <h1 class="page-title" itemprop="name headline">Research & Development</h1>
  
</div>

<div class="container content">
  <p>The following introduces research visions, and problems currently being
worked on.</p>

<ul id="markdown-toc">
  <li><a href="#research-vision" id="markdown-toc-research-vision">Research vision</a>    <ul>
      <li><a href="#science-of-intelligence-biotic-or-congnitive-complexity-through-adaptive-symmetries-breaking" id="markdown-toc-science-of-intelligence-biotic-or-congnitive-complexity-through-adaptive-symmetries-breaking">Science of intelligence: biotic or congnitive complexity through adaptive symmetries breaking</a></li>
      <li><a href="#embodied-intelligence-uncertainty-reduction-through-feedback-control-loop-with-environment" id="markdown-toc-embodied-intelligence-uncertainty-reduction-through-feedback-control-loop-with-environment">Embodied intelligence: uncertainty reduction through feedback-control loop with environment</a></li>
      <li><a href="#foundation-model-and-alignment-uncertainty-reduction-future-prediction-fast-and-slow" id="markdown-toc-foundation-model-and-alignment-uncertainty-reduction-future-prediction-fast-and-slow">Foundation model and alignment: uncertainty reduction (future prediction) fast and slow</a></li>
    </ul>
  </li>
  <li><a href="#research-problems" id="markdown-toc-research-problems">Research Problems</a>    <ul>
      <li><a href="#foundation-models-for-control" id="markdown-toc-foundation-models-for-control">Foundation models for control</a></li>
      <li><a href="#long-horizon-embodied-agent-alignment" id="markdown-toc-long-horizon-embodied-agent-alignment">Long-horizon embodied agent alignment</a></li>
      <li><a href="#superalignment-and-reinforcement-learning-fundamentals" id="markdown-toc-superalignment-and-reinforcement-learning-fundamentals">Superalignment, and reinforcement learning fundamentals</a></li>
    </ul>
  </li>
</ul>

<h3 id="research-vision">Research vision</h3>

<h4 id="science-of-intelligence-biotic-or-congnitive-complexity-through-adaptive-symmetries-breaking">Science of intelligence: biotic or congnitive complexity through adaptive symmetries breaking</h4>

<p>Previously, I have spent about 10 years investigating first-principle
derivation of Deep Neural Networks (DNNs), crystallizing the ideas in
two papers—<a href="https://zenodo.org/record/5814935">a letter</a>
introducing the main ideas, and a more <a href="https://zenodo.org/record/5814961">technical article</a> (where
most technicalities are put in the ~100 pages appendices).  An
introductory
<a href="/blog/2023/11/25/complexity-from-adaptive-symmetries-breaking/">blog</a>
is also written to introduce the article intuitively.</p>

<p>The basic idea is that Deep Neural Network (DNN) is a phenomenological model
of biotic intelligence, and DNNs could be studied similarly to physics through
symmetries, though the symmetries here are not conservative symmetries in
physics, but a symmetry perhaps that might form a different paradigm, and is
referred as <em>adaptive symmetry</em>. The learning process could be
formalized as a symmetries-breaking process where these symmetries are
broken to build a model of the world that could predict the future,
such that the uncertainty of survival and reproduction is decreased;
that is, to adapt to the world.</p>

<p>Those adaptive symmetries could be understood as choices that would be
made by us with equal chance—humans are organic system as well:</p>

<blockquote>
  <p>Two roads diverged in a yellow wood,<br />
And sorry I could not travel both. —  Robert Frost</p>
</blockquote>

<p>And chosen choices are broken symmetries.</p>

<p>More technically, there exists concentration of measures phenomena
induced by aforementioned adaptive symmetries for large DNNs that
enable learning of the network to stay within a phase where arbitrary
learning errors could be reduced.</p>

<p>Those errors could be understood as cognitive errors that lead to
mistakes (in the datasets, or environment). Or analogically, the wrong
choices we have made in life. And in an oversimplifying way, the less
errors we make, the more sophisticated situations we could handle, and
the more complex our model perhaps is.</p>

<p>And to summarize this process in the format of the catch phrases in
complexity science (e.g., “order from noise”), the training of organic
system might be referred as <em>biotic or cognitive complexity from
adaptive-symmetries breaking</em>.</p>

<h4 id="embodied-intelligence-uncertainty-reduction-through-feedback-control-loop-with-environment">Embodied intelligence: uncertainty reduction through feedback-control loop with environment</h4>

<p>The adaptive-symmetries breaking work sets the first step, or
foundation, for the subsequent efforts. More specifically, the
previous works analyze the simplest yet most fundamental setting,
i.e., object classification; it is conceptually clean, yet fundamental
intelligence arises from a being embodied in an environment whose
reduction of survival uncertainty requires building a feedback and
control loop (that is, daily feedback from environment and decisions
on what actions to take) with the environment.</p>

<p>Therefore, the next stage is to study realistic open systems that interact with
their environments. The potential complexity of the system and complexity of
the environment should be sufficiently large, such that a level that we denote
as intelligence in daily sense could be reached. This area of study is roughly
known as <em>Embodied Intelligence</em>.</p>

<p>There are two fitting pilot problems to study this area, for example. First,
the current breakthrough of large language model (LLM) is such an
uncertainty-reducing system in a semantic word worlds; the system is trained by
minimizing perplexity, another word for uncertainty. For LLM, the environment
is the Internet, and thus LLM agents are systems that interact with the
environment in the sense of embodied intelligence.  Second, the next
industrial-revolution scale innovation probably comes from cybernetic/control
systems such as self-driving cars, and those are classic embodied systems.
Those two problems are major challenges to realize the transformational impact
of artificial intelligence on societies.</p>

<h4 id="foundation-model-and-alignment-uncertainty-reduction-future-prediction-fast-and-slow">Foundation model and alignment: uncertainty reduction (future prediction) fast and slow</h4>

<p>The goal of decreasing uncertainty of reproduction is decomposed into
two problems by biotic intelligence by timescales: the prediction
under, and the prediction above hundreds of milliseconds. The
mechanism that predicts at the fast timescale is known as instincts,
and the mechanism that predicts at the slow timescale is known as
consciousness, reasoning, memory, and other modules function at our
daily life timescale. These are known as system 1 and system 2.</p>

<p>Therefore, the study of embodied intelligence is broken down into
studying these two mechanisms.  The frontier of the research thus has
reached the state where we have a system close to human instincts in
language processing—this is known as pretrained <em>Foundation Models</em>. And
the slow system is being investigated largely under the name of <em>Agent</em>,
which requires long horizontal reasoning and interaction with the
Internet, and also involves system 2 concepts such as long term memory.
The bridge of the two is investigated under the name of <em>Alignment</em>,
where the foundation model is aligned to human intentions, and becomes
an agent.</p>

<p>Some clarification might be needed here, given that alignment has a
connotation to safety issues.  The developmental progress of an
organism is a process where it explores possible actions in the
environment, and accumulates a repository of <em>action sequences</em> such
that it could be more fit to perpetuate in the environment. Therefore,
to align a model is to align this developmental progress, and thus the
capability alignment and human value alignment (i.e., the common
denotation of “aligning” catastrophic superintelligence) are deeply
intertwined. They both might be studied under the theme to find the
right formalism of interaction between the environment (e.g.,
Internet) and the model/agent.</p>

<h3 id="research-problems">Research Problems</h3>

<p>More concretely, some concrete angles that we are working on are
briefly introduced in the following.</p>

<h4 id="foundation-models-for-control">Foundation models for control</h4>

<p>To move beyond the word world, current foundation models lack
instincts in the physical world. Large language model builds this kind
of instinct through pretraining on trillions of worlds, and this
instinct is an abstraction of the word world. However, this way of
building abstract over word world might not naively generalize to
other categories of signals. Natural words are already coarse-grained (in other
words, semantic) information packet of the words, and thus predicting
next-word sidesteps the problem of building coarse-grained abstraction
of the world from raw signals (e.g., pixels). An intelligence in the
physical world (e.g., autonomous robots) needs somehow to have a
mechanism to build abstraction from raw signals that are directly wired
with control. A foundation model for control needs such abstraction
capability, and this abstraction might manifest as animal-level
instinct to navigate in the physical world.</p>

<h4 id="long-horizon-embodied-agent-alignment">Long-horizon embodied agent alignment</h4>

<p>A significant capability of the slow system manifests as long-horizon
decision or reasoning capability. We study this problem under the
setting of LLM.  As discussed previously, next-word prediction might
be understood as something that might be called instincts, and it
lacks long-horizon decision capability to be useful as an competent
assistant (e.g., one that we could truly delegate complex tasks). To
make a language model “understand” our request, a mechanism of
feedback and long-chain reasoning need to be incorporated. Currently,
this ability is nascent in language model, and we are studying how to
enable long-horizon reasoning and decision that require
tool usage in LLM. This is probably what Q* or Gemini aims for.</p>

<h4 id="superalignment-and-reinforcement-learning-fundamentals">Superalignment, and reinforcement learning fundamentals</h4>

<p>The previous directions focus more on the problem side, and the previous
problems likely would enable the new development of reinforcement learning
(RL) algorithms. Intuitively, humans could make skills instinctive
through deliberate practice, and also come up with high quality
solution through careful reasoning and self critique. We also works on
formalism of those two behaviors. For the latter, it is roughly known
as language feedback or AI feedback for alignment. The idea is that
generation is a NP problem, while discrimination is a P problem, and
thus a good enough model could self-critique and self-improve. In
addition to enabling scalable alignment, this might provide solution
to hard RL problems such as sparse reward, and instability in RL
training.</p>

</div>

    </section>

    

    <nav class="navbar navbar-default navbar-fixed-bottom">
<div class="container footer-content">
    <!-- Nothing is there. -->
</div>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
</nav>

  </body>

</html>
