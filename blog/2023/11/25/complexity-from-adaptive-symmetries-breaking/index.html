<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta content="Shawn" property="og:site_name">
  <link href="/favicon.png" rel="icon">

  
  <meta content="Complexity from Adaptive Symmetries Breaking: Mathematical Principle of Organic World" property="og:title">
  

  
  <meta content="article" property="og:type">
  

  
  <meta content="<h1>Shawn W. M. Li</h1> " property="og:description">
  

  
  <meta content="http://0.0.0.0:4000/blog/2023/11/25/complexity-from-adaptive-symmetries-breaking/" property="og:url">
  

  
  <meta content="2023-11-25T21:40:17+08:00" property="article:published_time">
  <meta content="http://0.0.0.0:4000/about/" property="article:author">
  

  <meta property="og:image" content="">

  
  
  <meta content="Research" property="article:section">
  
  

  
  
  

  <title>Complexity from Adaptive Symmetries Breaking: Mathematical Principle of Organic World - Shawn</title>
  <meta name="description" content="In this article, we give an intuitive introduction to the article (Li, 2022) that gives a first-principlederivation of Deep Neural Networks.The basic idea is...">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
  <!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/css/bootstrap.min.css" integrity="sha384-GJzZqFGwb1QTTN6wy59ffF1BuGJpLSa9DkKMp0DgiMDm4iYMj70gZWKYbI706tWS" crossorigin="anonymous"> -->
  <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.slim.min.js"></script> -->
  <!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script> -->
  <!-- <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script> -->
  <link rel="stylesheet" href="/css/main.css">

  <!-- Typesetting math using MathJax -->
  <!-- mathjax config similar to math.stackexchange -->
  <script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     jax: ["input/TeX", "output/HTML-CSS"],
     tex2jax: {
       inlineMath: [ ['$', '$'] ],
       // displayMath: [ ['$$', '$$']],
       processEscapes: true,
       skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
     },
     messageStyle: "none",
     "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
   });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-44220731-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>



  <link href="/css/fonts/orbitron.css" rel="stylesheet" type="text/css">
  <link href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
  <link rel="canonical" href="http://0.0.0.0:4000/blog/2023/11/25/complexity-from-adaptive-symmetries-breaking/">
  <link rel="alternate" type="application/rss+xml" title="Shawn" href="http://0.0.0.0:4000/feed.xml">
</head>


  <body>
    <section>
  <nav class="navbar navbar-default navbar-expand-lg fixed-top">
    <div class="container">

      <!-- Brand and toggle get grouped for better mobile display -->
      <a class="navbar-brand" href="/">Shawn</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navBar">
        <i class="fa fa-bars"></i>
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" align="right" id="navBar">
        <ul class="navbar-nav ms-auto">
          <li class="nav-item"><a class="nav-link" href="/about">About</a></li>
          <li class="nav-item"><a class="nav-link" href="/research">Research & Development</a></li>
          <li class="nav-item"><a class="nav-link" href="/blog">Blog</a></li>
          <!-- <li class="nav-item"><a class="nav-link" href="/bio">Biography</a></li> -->
          <!-- <li class="nav-item"><a class="nav-link" href="/essays">Essays</a></li> -->
          <li class="nav-item"><a class="nav-link" href="/community">Community</a></li>
        </ul>
      </div><!-- /.navbar-collapse -->

    </div>
  </nav>
</section>


    <section>
      <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
    <div class="container">
      <h1 class="page-title" itemprop="name headline">Complexity from Adaptive Symmetries Breaking: Mathematical Principle of Organic World</h1>
      <p class="post-meta"><time datetime="2023-11-25T21:40:17+08:00" itemprop="datePublished">Nov 25, 2023</time></p>

</div>


  <div class="content post-content container" itemprop="articleBody">
    <p>In this article, we give an intuitive introduction to the article <a class="citation" href="#shawn_w_m_li_2022_5814935">(Li, 2022)</a> that gives a first-principle
derivation of Deep Neural Networks.</p>

<p>The basic idea is that Deep Neural Network (DNN) is a phenomenological model
of biotic intelligence, and DNNs could be studied similarly to physics through
symmetries, though the symmetries here are not conservative symmetries in
physics, but a symmetry perhaps that might form a different paradigm, and is
referred as <em>adaptive symmetry</em>. The learning process could be formalized as a
symmetries-breaking process where these symmetries are broken to build a model
of the world that could predict the future, such that the uncertainty of
survival and reproduction is decreased; that is, to adapt to the world.</p>

<p>If we consider the conservative symmetry in physics as the
mathematical principle of physics (i.e., natural world), then from the
same philosophy of science we could argue that the adaptive symmetry
is the mathematical principle of organic world.</p>

<p>More technically, there exists concentration of measures phenomena for large
DNNs that enable learning of the network to stay within a phase where arbitrary
learning errors could be reduced. That is, biotic or cognitive complexity
arises from adaptive-symmetries breaking.</p>

<p>The “large” DNN here refers to a relative relationship between dataset
and model capacity, and thus this phenomenon lets us speculate
scaling the model and the dataset, and logically leads to the current
marvelous large language models. And it further suggests that the next
conceptual breakthrough might be in the field of embodied
intelligence, which include <em>Agent</em> on Internet.</p>

<p>In the following, we shall break down the ideas step by step.</p>

<ul id="markdown-toc">
  <li><a href="#the-intelligibility-of-intelligence" id="markdown-toc-the-intelligibility-of-intelligence">The intelligibility of intelligence</a></li>
  <li><a href="#the-epistemic-foundation-of-physics" id="markdown-toc-the-epistemic-foundation-of-physics">The epistemic foundation of physics</a></li>
  <li><a href="#conservation-symmetry-breaking-the-mathematical-principle-of-changes-in-the-natural-world" id="markdown-toc-conservation-symmetry-breaking-the-mathematical-principle-of-changes-in-the-natural-world">Conservation symmetry breaking: the mathematical principle of changes in the natural world</a></li>
  <li><a href="#neural-network-training-solves-a-neural-differential-equation" id="markdown-toc-neural-network-training-solves-a-neural-differential-equation">Neural network training solves a neural differential equation</a></li>
  <li><a href="#neural-circuits-the-atom-of-intelligence" id="markdown-toc-neural-circuits-the-atom-of-intelligence">Neural circuits: the atom of intelligence</a></li>
  <li><a href="#circuit-symmetry-symmetry-of-organism" id="markdown-toc-circuit-symmetry-symmetry-of-organism">Circuit symmetry: symmetry of organism</a></li>
  <li><a href="#adaptive-symmetries-breaking-the-mathematical-principle-of-the-organic-world" id="markdown-toc-adaptive-symmetries-breaking-the-mathematical-principle-of-the-organic-world">Adaptive symmetries breaking: the mathematical principle of the organic world</a></li>
  <li><a href="#adaptive-symmetries-and-large-language-models" id="markdown-toc-adaptive-symmetries-and-large-language-models">Adaptive symmetries and large (language) models</a></li>
  <li><a href="#adaptive-symmetries-and-embodiedlanguage-intelligence" id="markdown-toc-adaptive-symmetries-and-embodiedlanguage-intelligence">Adaptive symmetries and embodied/language intelligence</a></li>
</ul>

<!-- more -->

<h4 id="the-intelligibility-of-intelligence">The intelligibility of intelligence</h4>

<p>Albert Einstein once said “the most incomprehensible thing about the
world is that it is comprehensible”. Human being has come a long way
to understand our world: to paraphrase Carl Sagan, we have come to
know the machinery that generates the sunlight that makes life
possible, the gravity that glues us to Earth that would otherwise
send us spinning off into space, or the atoms of which we are made and
on whose stability we fundamentally depend.</p>

<p>For anyone have thought about it deeply, this is a miraculous
achievement of mankind. Nowadays, people are not amazed about the
intelligibility of physical world. But before the Enlightenment, the
physical world was considered as the creation of gods, and not
comprehensible to mortals.</p>

<p>Each era has its own epistemic struggles, and currently, humanity are
facing another philosophical problem in a similar nature; that is, the
intelligibility of intelligence <a class="citation" href="#Salamon">(Salamon &amp; Rayhawk, 2010)</a>. This post aims to
explain a plausible epistemic foundation where the intelligibility of
intelligence is possible. And to begin with, we need to revisit the
intelligibility of physics.</p>

<h4 id="the-epistemic-foundation-of-physics">The epistemic foundation of physics</h4>

<p>The intelligibility of natural world comes from a concept known as
symmetries. P. Anderson, the Nobel laureate, described in the seminal
essay <em>More is Different</em> that, “it is only slightly overstating the
case to say that physics is the study of symmetry”. Symmetries are
repeated structure that are so regular that it enables synchronized
microscopic phenomena to be observable across scales at the
macroscopic scale. This similarity across scales is referred as
<em>self-similarity</em>. To give an example, a snowflake would repeat its
six-fold symmetries indefinitely, and its microscopic structure (at
the nanometer scale) would manifest as the macroscopic crystal (at the
centimeter scale) as we see it.</p>

<figure style="text-align: center;">
<img src="/assets/six_fold_symmetry.gif" style="width: 30%;" />
<img src="/assets/snow_flake.webp" style="width: 30%;" />
<br />
<caption> Six-fold symmetry and snowflakes. </caption>
</figure>

<p>Symmetry is the structure that bridges scales; it induces stable
macroscopic structure from the microscopic, and let the microscopic
worlds be measurable and thus knowable to us. The whole edifice of
physics is built upon identifying symmetries existing and formalizing
them into laws.</p>

<h4 id="conservation-symmetry-breaking-the-mathematical-principle-of-changes-in-the-natural-world">Conservation symmetry breaking: the mathematical principle of changes in the natural world</h4>

<p>These symmetries are not a static concept, but are equilibria of
dynamic motion of objects (e.g., molecules motions) that <em>conserve</em> a
quantity called <em>free energy</em>. And when those symmetries are broken, we
would observe phenomena that we call <em>change</em>.</p>

<p>To give an example, when temperature rises, the heat perturbations of
the molecule oscillation could not be contained, and would induce a
synchronized/cascaded change across all molecules. As a result, the
six-fold symmetries of water molecules would break into a symmetry
referred as the rotation symmetries, and the symmetry-breaking process
is known as <em>phase change</em> in physics. And macroscopically, water
molecule collectively is observed as water sphere (under zero gravity).</p>

<figure style="text-align: center;">
<img src="/assets/water.webp" style="width: 30%;" />
<br />
<caption> Rotation symmetry and water. </caption>
</figure>

<p>And our universe is arguably an almost infinitely long symmetry
breaking process of high energy particles created at <em>Big Bang</em>, the
creation of the universe. This is why P. Anderson said that “it is
only slightly overstating the case to say that physics is the study of
symmetry”. And physically, our worlds might be just symmetries and
their breaking.</p>

<h4 id="neural-network-training-solves-a-neural-differential-equation">Neural network training solves a neural differential equation</h4>

<p>If we make the changes through symmetries breaking almost indefinitely
complex and temporally extended, we reach something that might be
quite close to the phenomena that we call <em>life</em>. And further
complexification of this diachronic symmetry breaking process might
create something called <em>neural plasticity</em>—more details could be
found in this article <a class="citation" href="#shawn_w_m_li_2022_5814935">(Li, 2022)</a> introduced in the <a href="/research/#science-of-intelligence">science of
intelligence</a> section on the
<a href="/research">research</a> page. In the following sections, we introduce
the symmetries breaking in DNNs intuitively.</p>

<p>DNN is a phenomenological of cognition, and the training of a DNN is a
process of change where relevant information (related to survival or
reproduction) is encoded in the network.  To explain the motivation to
scale DNNs, Ilya Suskever <a href="https://www.youtube.com/watch?v=Ft0gTO2K85A">explains in an
interview</a> that training
a DNN is like solving a neural equation. More concretely, gradient
descent actually solves a highly sophisticated nonlinear differential
equation,</p>

\[\nabla \mathcal{L}(W\sigma \ldots W\sigma X, y) = 0,\]

<p>where $W, \sigma$ denotes the weights and activation function respectively.
The larger the network, the more degree of freedom exists such that a
better solution could be reached by gradient descent.</p>

<p>This is also a formalism of the <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">Bitter
Lesson</a> by
Rich Sutton: learning and search are the two most important classes of
techniques in AI research. By local search in the weight space, a
network is found that minimizes the error $\mathcal{L}$ (i.e., the
gradient is zero when a local minimum is found).</p>

<h4 id="neural-circuits-the-atom-of-intelligence">Neural circuits: the atom of intelligence</h4>

<p>Further extrapolating the mathematical principle of change in the
natural world, we could reason that the intelligibility of
intelligence might require us to find stable characteristics in those
changes that are measurable across scales. And in <a class="citation" href="#shawn_w_m_li_2022_5814935">(Li, 2022)</a>, we find something referred as <em>circuit
symmetries</em>.</p>

<p>Though the characterization is given by highly sophisticated
mathematical formalism, the basic idea behind circuit symmetries is
simple. First, we explain what a circuit is—in DNNs, it serves
as the role of atom in physics.</p>
<figure style="text-align: center;">
<img src="/assets/fig_2_dnn_graph.png" style="width: 80%;" />
</figure>
<p>As illustrated above, at initialization of a DNN, each weight (the
lines connecting dots) is randomly initialized, and each neuron (the
dots) would be activated also randomly. As a result, if we connect the
input neuron (from the left) to the output neuron (to the right),
illustrated as the green line, we would get an ordered set and we
refer this set as a <em>circuit</em>.</p>

<p>More formally (though still simplified),
let $\boldsymbol{I}_{l}$ denotes $\{1, \ldots, n_l \}, l \in \{1,
\ldots, L\}$, where $n_l$ denotes the number of neurons of the
network and $L$ is the number of layers,
(c.f. the above illustration), and $l \in \{1, \ldots, L\}$ is the
layer index up to $L$.
Let $\boldsymbol{I}$ denote $\boldsymbol{I}_1 \otimes \ldots \otimes \boldsymbol{I}_L$,
where $\otimes$ denotes Cartesian product.
Then $\boldsymbol{I}$ denotes the indices of all circuits, and we could denote a
circuit as</p>

\[\Psi_{\boldsymbol{i}} = X_{i_1}W_{i_1 i_2}H_{i_2}\ldots W_{i_{L-1} i_{L}}H_{L},\]

<p>where $\boldsymbol{i} \in \boldsymbol{I}$, $X_{i_1}$ is the input
neuron $i_1$, $W_{i_1 i_2}$ is the weight connect neuron $i_1,
i_2$, $H_{i_2}$ is the hidden neuron $i_2$, and so on.</p>

<p>To give simplistic understand of the circuit’s behaviors, we might
understand each neuron as a gate that decides whether the circuit that
it belongs is activated (and thus a circuit is controlled by multiple
gates), and the weights are scalars that multiplied together when this
circuit activates.</p>

<p>By looking at the network as an ensemble of neural circuits, we could
see symmetries arise. Circuits play the role of atoms in physics, and
later neuron ensemble is just the summation of those circuits, just
like the magnetic force is in a sense the summation of individual atom
magnetic field.</p>

<h4 id="circuit-symmetry-symmetry-of-organism">Circuit symmetry: symmetry of organism</h4>

<p>Recall that symmetries are repeated stable structure at microscopic
scale that manifests macroscopic behaviors. The green line previously
is one circuit; however, each possible black line previously is also a
circuit. As a result, the behaviors of the output (blue)
neuron—which is the summation of each circuit’s value (the
multiplication of all weights of the circuit when it activates)—is
the collective behaviors of all the circuits. We refer this collective
(which mathematically is a summation) as a <em>neural assembly</em>. And
there is a microscopic symmetry of circuits that manifests as a
macroscopic symmetry of the neural assembly, and this composite
symmetry at circuit level (instead of at atom level) might be the key
to understand biotic systems (i.e., organism). We introduce the
symmetry as follows.</p>

<p>The snowflake is the macroscopic ensemble behavior of the six-fold
symmetry of ice molecules’ the quantum statistical wave (distribution)
function. Marvelously, the probability distribution of possible values
of the neural assembly is self-similar to the probability distribution
of the circuits’ value.</p>

<p>More specifically, at the beginning of network training, due to the
fact that weights are initialized according to symmetrical probability
distributions (e.g., Gaussian distribution), the circuits would be of
a symmetrically distributed distribution in the PAC (probably
approximately correct) sense (mathematically proved and experimentally
verified in the paper). We refer this as <em>circuit symmetries</em>: that is
to say, at the beginning of the training, the circuits would
contribute symmetrically to the output neuron.</p>

<figure style="text-align: center;">
<img src="/assets/fig_3b_basis_circuits_merged.png" style="width: 100%;" />
<br />
<caption> Circuit symmetry: the value of the circuits distributed symmetrically. </caption>
</figure>

<p>The circuit symmetry (including the broken circuit symmetry introduced
later) is given as a theorem in the article <a class="citation" href="#shawn_w_m_li_2022_5814935">(Li, 2022)</a>, which is proved by proving convergence
by characteristic function between the circuit distribution and a
symmetric distribution (theorem 1 in the paper), and looks something
like this</p>

\[\forall g_t \in G_t,  \left| \mathbb{E}\left[ e^{i(\frac{\sqrt{n}}{c})^{e-s}\Psi^{\overline{l}}_{g_t.\boldsymbol{i}}t}\right] - \mathbb{E}\left[e^{i(\frac{\sqrt{n}}{c})^{e-s}\hat{\Psi}^{\overline{l}}_{g_t.\boldsymbol{i}}t}  \right]\right| \\
            \leq \left| O\left( (\frac{1}{2\sqrt{n}})^{e-s-b_{\boldsymbol{i}}}\sin(t) \right) \right|,\]

<p>where $\Psi$ is a formalism for circuits in the article <a class="citation" href="#shawn_w_m_li_2022_5814935">(Li, 2022)</a>. The bound here is not intended to be
readable, and interesting readers may refer to the paper.</p>

<p>And we could mathematically prove and experimentally verify that the
coarse-graining (or summation) of those symmetrically distributed
circuits (in which, for example, the output neuron is literally a
neural assembly) is also symmetrically distributed
probabilistically in the PAC sense—though we would only done this
for some types of the neuron assemblies that are relevant in the paper. We
show the distribution of neural assemblies in the following
plots—the plots shown here are not distribution of neuron output,
but gradient and Hessian entries, which are also neuron assemblies and
are more relevant in the paper.</p>

<figure style="text-align: center;" id="neuron-assembly">
<img src="/assets/fig_4a_gradients_distribution.png" style="width: 40%;" />
<img src="/assets/fig_7a_Hessian_mean_distribution.png" style="width: 40%;" />
<br />
<caption>Distribution of neural assemblies</caption>
</figure>

<p>More formally, a neuron assembly is just the summation of all circuits
in the assembly,</p>

\[\sum_{\boldsymbol{i} \in \boldsymbol{I}}\Psi_{\boldsymbol{i}} = X_{i_1}W_{i_1 i_2}H_{i_2}\ldots W_{i_{L-1} i_{L}}H_{L},\]

<p>and the summation/coarse-graining over $n^{L}$ (we assume all layer
has $n$ neurons for simplicity) maintain the symmetry distribution in
a PAC sense—here we use the output neuron as an example, and a
neuron assembly could be other assemblies other than the output neurons.</p>

<p>Therefore, the symmetry of circuit is self-similar to, and manifests
at macroscopic as the symmetry of neural assemblies.</p>

<p>Lastly, we complement with the distribution of weights throughout
training, whose symmetry induces the circuit symmetry to corroborate.</p>

<figure style="text-align: center;" id="weight_symmetry">
<img src="/assets/fig_3a_weight_distribution_unnormalized.png" style="width: 60%;" />
<br />
<caption>The weight distribution throughout training</caption>
</figure>

<h4 id="adaptive-symmetries-breaking-the-mathematical-principle-of-the-organic-world">Adaptive symmetries breaking: the mathematical principle of the organic world</h4>

<p>As the symmetry-breaking in the natural/inorganic world induces
macroscopic changes, the circuit symmetry breaking of neural networks (an
organic system) induces macroscopic changes as well; however, such
changes are not physical, but informational.</p>

<p>Imagine that we perturb the weights (i.e., the green line), this
perturbation would direct influence the output neurons (i.e., the blue
neuron on the right), assuming the circuit activates. And we could
freely increase or decrease the weight; as a result, we could freely
increase the contribution of this circuit to the output neuron, or
decrease it.</p>

<p>Further imagine that the weights are perturbed systematically by <em>back
propagation</em>. Then, this system of degree of freedom (i.e., weights)
would collectively be optimized to satisfy the constraint given by the
labels through the objective function. As a result, the
random/symmetric contributions of those circuits are modified
collectively to minimize the objective function (which characterizes
training errors).</p>

<p>Therefore, a DNN is a system poses at the state of <em>adaptive symmetry</em>
to minimize the errors to fulfill its goal characterized by the
objective function: at the beginning of the training, the
contributions of circuits are symmetrically distributed to increase or
decrease the errors; through the feedback from the objective, this
symmetries are gradually broken to reduce the errors.</p>

<p>To give an example, the following training errors (prediction
accuracy) are a network trained to classify images into object
categories. As training progress, the network could classify all
object correctly in the training set. And the weight’s distribution,
shown in the earlier <a href="#weight_symmetry">figure</a>, slightly
and gradually skews during training.</p>

<figure style="text-align: center;">
<img src="/assets/fig_8b_cifar10_loss.png" style="width: 40%;" />
<img src="/assets/cifar_dataset.png" style="width: 40%;" />
<br />
<caption>(Left) Training errors and prediction accuracy throughout
training. (Right) CIFAR10 image classification task.</caption>
</figure>

<p>Therefore, the emergence of informational structure (which is often
complex, and in the previous example, recognizes objects) in
organic/neural system is a process of adaptive symmetries breaking.</p>

<h4 id="adaptive-symmetries-and-large-language-models">Adaptive symmetries and large (language) models</h4>

<p>The symmetries in physics informs us the <em>control</em> and <em>order parameters</em>
of the system; for example, the temperature controls the phase of the
water molecule system, and when it crosses a threshold, the order of
the system changes from liquid phase to the snow/solid phase. The
symmetries of biotic/neural system in this case also informs us about
the phases of the system: it informs us about the <em>adaptability</em>, or
<em>plasticity</em> of the system. And all those analysis in <a class="citation" href="#shawn_w_m_li_2022_5814935">(Li, 2022)</a> gives a formal, theoretical and
experimental (which combined is referred as scientific from the
philosophy of science) characterization that helps us understand the
scaling of the neural network model.</p>

<p>Previously, we describe that circuit symmetries exists in a DNN, and
the breaking of those symmetries reduces the training errors. Notice
that given a $L$ DNN with width $n$ for each layer, there are
exponentially number $n^{L}$ of circuits w.r.t. to weights. Thus, we
might speculate that there are a very large possibilities where the circuits
could be perturbed to reduce the errors, subject to the interaction or
constrains among the neurons (because the change of one weight could
also influence exponential number of circuits as well).</p>

<p>In <a class="citation" href="#shawn_w_m_li_2022_5814935">(Li, 2022)</a>, we formally show that when
there are certain diversity among neurons, despite the circuit
symmetries would be broken during training, their macroscopic adaptive
symmetry would not be broken. As a result, there always exist
directions where the output of the final neuron could be modified to
minimize the errors. More technically, the adaptive symmetries
manifests in the Hessian of the objective function as the symmetry of
eigenvalue distributions, as shown in the following <a href="#adaptive-symmetry">Hessian
figure</a>. Recall that when there are negative
eigenvalue exists for a function, all stationary points are saddle
points, and gradient descent direction exists to further minimize the
function.</p>

<figure style="text-align: center;" id="adaptive-symmetry">
<img src="/assets/fig_8a_cifar10_eigenspectrum.png" style="width: 40%;" />
<br />
<caption>Eigenvalue distribution throughout training.</caption>
</figure>

<p>The symmetry of eigenvalue distribution is given as theorem 2 in the
article <a class="citation" href="#shawn_w_m_li_2022_5814935">(Li, 2022)</a>, which is proved by
convergence of probability distribution on matrices, and looks something
like this,</p>

\[P\left(  \left|  \text{tr }\left( \boldsymbol{B} \boldsymbol{G}(z) - \boldsymbol{M}(z) \right) \right|
    \leq ||\boldsymbol{B}|| \frac{N^{\epsilon}}{N\Im z} \right)
    \geq  1 - CN^{-D},\]

<p>where $G$ is the resolvent of the Hessian matrix, and $M$ is the
resolvent of a matrix with symmetric eigenvalue distribution. The
bound here is not intended to be readable, and interesting readers may
refer to the paper.</p>

<p>In this case, the order parameter of the DNN system is the
distribution of gradients, which is also symmetric throughout training
(c.f. <a href="#neuron-assembly">the gradient distribution figure</a>). Recall
that nonzero gradient implies directions to reduce errors; this
characterizes the order of DNN, i.e., adaptability or plasticity.  As
acute readers might notice, compared with the single scalar that
characterizes the order of the simple physical system like water, the
order parameter of the DNN system is high dimensional. The philosophy
of science that defines order parameter is discussed in <a class="citation" href="#shawn_w_m_li_2022_5814935">(Li, 2022)</a>, and we would not delve into them here.</p>

<p>Similarly, the control parameter is also not a single scalar in the
case of the simple physical system like water molecule system, but is
high dimensional as well, and is formalized with rather complicated
math—interested readers could read the paper. Here, we demonstrate
the control parameter visually. The control parameter characterizes
the interaction among neuron assemblies, and as training progresses and circuits
breaks their symmetries, the interaction among assemblies increases, as
shown in the following, where each pixel represents the correlation
among neural assemblies (more technically Hessian entries).</p>

<figure style="text-align: center;" id="neuron-assembly">
<img src="/assets/fig_6b_covariance_matrix_start.png" style="width: 40%;" />
<img src="/assets/fig_6c_covariance_matrix_end.png" style="width: 40%;" />
<br />
<caption>Correlations among neuron assemblies at the beginning (left)
and the end of the training (right).</caption>
</figure>

<p>More technically, the control parameters are coupling density among
neural assemblies. To give an over-simplified demonstration, let
$\mathbb{I}$ denote an index set indexing neuron assemblies; denote
 $\mathcal{N}(\alpha)$ the set of assemblies that couple with the
 assembly $\alpha \in \mathbb{I}$. For theorem 2 to hold, each
 $\alpha \in \mathbb{I}$ needs to satisfy $|\mathcal{N}(\alpha)| &lt;
 \sqrt{N}$, where $N$ is the number of parameters of the network; that
is, $\alpha$ is at most statistically dependent with $\sqrt{N}$
neuron assemblies. The condition on control parameters are given as
assumptions in the paper, and this constraint on coupling density is a
simplified version of assumption 2 in the article <a class="citation" href="#shawn_w_m_li_2022_5814935">(Li, 2022)</a>. The neuron assemblies here are
Hessian entries, and interested readers may refer to the article for details.</p>

<p>We give a rather toy outline of proof here, and interested readers may
refer to the paper. The basic logic is that the circuit symmetry leads
to some stable characteristics among the neuron assemblies—recall
that at the very beginning we have discussed that stable
characteristics induced by symmetries bridge the microscopic and the
macroscopic. By characterizing the stable characteristics, we make
some restriction on the interaction among the neuron assemblies; this
leads to the control parameters and is formulated as assumptions;
Then, those restriction would enable us to prove the probability bound
on the Hessian’ eigenvalue distribution, which is symmetric in the PAC
sense.</p>

<p>Recall that neuron circuits start with input neurons, which are just
data.  Therefore, the interaction could be roughly understood to
encode the information in the dataset with hierarchical
patterns. Therefore, the quantitative characterization (of control
parameter) involves both the dataset and the model size: more
specifically, it is an interplay between the <em>complexity of the
dataset</em>, and the potential complexity of the information the network
is able to encode, or in other words, <em>network capacity</em>. In this
case, the interaction are still rather sparse given that the dataset
is well beyond the network capacity. As a result, there are sufficient
circuit symmetries that enable the network to reach zero errors.</p>

<p>To summarize, we have given a formalism of DNN training, and validate
it experimentally. The formalism and experiments show that</p>
<ol>
  <li>the training of a DNN solves a neural differential equation;</li>
  <li>the training process finds a solution that satisfies the
constraints dictated by the dataset (and the objective function);</li>
  <li>the training process is a circuit-symmetry breaking process;</li>
  <li>however, for a sufficiently large network, whose network capacity is
well below the complexity of the dataset, even if some of the
circuits break their symmetry, there are sufficient reservoir of
circuits, the macroscopic behavior still manifests as adaptive
symmetry that enables the network to reach zero training errors.</li>
</ol>

<p>This straightforwardly lets us extrapolate the situation, what if we
could increase the complexity of the dataset and the capacity of the
network indefinitely? We have known that DNN has remarkable
generalization capacity (though this is also an interesting problem to
study scientifically), and thus a lower training error accompanies
lower test error. This leads to us to speculate scaling the model and
the dataset, and logically leads to the current marvelous large
language models. And it further suggests the next conceptual
breakthrough might be in the field of embodied intelligence.</p>

<p>Nonetheless, this could be argued as merely posterior insight. The
point is that the formalism here could guide us to find the next
conceptual breakthrough. Though this might be a topic of another day,
we shall briefly discuss it as an epilogue.</p>

<h4 id="adaptive-symmetries-and-embodiedlanguage-intelligence">Adaptive symmetries and embodied/language intelligence</h4>

<p>The interplay between the complexity of dataset and the model
capacity previously is not a static concept but rather a feedback control loop
between the environment (i.e., the dataset) and the agent (i.e., the
model). Thus, the extrapolation moves beyond scaling language model
(which happened in the past a few years).</p>

<p>What is happening in the past almost 100 years of intelligence
research might be something like the following. During the evolution
history, our intelligence gets encoded into our genes, and that
becomes our foundation model. Our lifetime is an
adaptive/alignment/epigenetic developmental process with the embodied
environment. And in the past 100 years, we have just accumulated
enough digital environmental information to encode a foundation model
(e.g., ImageNet and Common Crawls) into an artificial neural network.</p>

<p>Yet, true intelligence is not created by training entirely over those
offline data; it requires online interaction with the
environment. Therefore, the result here further suggests studying
embodied intelligence, which does not narrowly restricted in settings
such as robotics or autonomous vehicles, but also agents
embodied on the Internet.</p>

<p>Therefore, though the fundamental principle probably would hold well
into the future, the formalism in the article <a class="citation" href="#shawn_w_m_li_2022_5814935">(Li, 2022)</a> is probably a preliminary form, and the
next step is to study large models interacting with sophisticated
environment. And further research is lightly discussed in the
<a href="/research">research</a> page.</p>

<ol class="bibliography"><li><span id="shawn_w_m_li_2022_5814935">Li, S. W. M. (2022). <i>Complexity from Adaptive-Symmetries Breaking: 
                   Global Minima in the Statistical Mechanics of Deep
                   Neural Networks (Letter Version)</i> (Version v1). Zenodo. https://doi.org/10.5281/zenodo.5814935</span></li>
<li><span id="Salamon">Salamon, A., &amp; Rayhawk, S. (2010). How Intelligible is Intelligence ? <i>ECAP10: VIII European Conference on Computing and Philosophy</i>.</span></li></ol>

  </div>

</article>

    </section>

    

    <nav class="navbar navbar-default navbar-fixed-bottom">
<div class="container footer-content">
    <!-- Nothing is there. -->
</div>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
</nav>

  </body>

</html>
